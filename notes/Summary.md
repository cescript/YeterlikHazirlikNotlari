<p style="text-align: center; font-family:cursive; page-break-before: always">TEMEL KAVRAMLAR</p>

â© DoÄŸrusal **baÄŸÄ±msÄ±z** vektÃ¶r setinde her vektÃ¶r vektÃ¶rÃ¼n kapsadÄ±ÄŸÄ± uzayÄ± geniÅŸletir dolayÄ±sÄ±lya  $p_1 v_1 + p_2 v_2 + \dots + p_n v_n \neq 0,  \forall p_i \in \mathbb{R} {\setminus \{0 \}}$ olmalÄ±dÄ±r.

â© Birbirine **dik** olan iki vektÃ¶r **ortogonaldir** ve $\mathbf{x}^{\intercal} \mathbf{y} = 0$ dÄ±r. TÃ¼m **ortogonal** vektÃ¶rler aynÄ± zamanda **doÄŸrusal baÄŸÄ±msÄ±z**dÄ±r.

DoÄŸrusal baÄŸÄ±msÄ±z vektÃ¶rlerden ortogonal vektÃ¶rler elde etmek iÃ§in **Gram-Schmidt YÃ¶ntemi** kullanÄ±labilir. $q_1 = v_1$ den baÅŸlanarak, her adÄ±mdayeni bir Ã¶nceki vektÃ¶rlere dik olan vektÃ¶rler elde edilir: $q_n = v_n - \sum_{i=1}^{n-1}\frac{\langle v_n,v_i \rangle}{\langle v_i,v_i \rangle} v_i$

$$
\boxed{\lVert \mathbf{x} \lVert_p = ( \sum \lvert x_i \lvert )^{1/p}} \quad \boxed{\lVert \mathbf{A} \lVert_p = \max_{x \neq 0} \frac{\lVert \mathbf{A}\mathbf{x} \lVert_p}{\lVert \mathbf{x} \lVert_p}} \quad \boxed{\text{cond}(\mathbf{A}) = \lVert \mathbf{A} \lVert \lVert \mathbf{A}^{-1} \lVert}
$$

â© $\mathbf{A} \mathbf{v} = \lambda \mathbf{v}$ ÅŸartÄ±nÄ± saÄŸlayan $\mathbf{v}$ vektÃ¶rlerine $\mathbf{A}$'nÄ±n Ã¶zvektÃ¶rleri, $\lambda$ katsayÄ±sÄ±na ise Ã¶zdeÄŸeri denir.

<blockquote>

ğŸ“Œ $\mathbf{A} \mathbf{v} - \lambda \mathbf{v} = 0$ olduÄŸundan $\mathbf{v} \neq 0$ iÃ§in $\text{det}(\mathbf{A} - \lambda \mathbf{I}) =0$ olmalÄ±dÄ±r.

ğŸ“Œ Ã–zdeÄŸerlerin **toplamÄ±** matrisin **izine** $\sum_i \lambda_i = \textbf{Tr}\{\mathbf{A}\}$, **Ã§arpÄ±mÄ±** **determinantÄ±na** $\prod_i \lambda_i = \lvert \mathbf{A} \lvert$ eÅŸittir.

ğŸ“Œ GerÃ§el ve **simetrik** bir $\mathbf{A}$ matrisinin **birbirinden farklÄ± Ã¶zdeÄŸerlere sahip tÃ¼m Ã¶zvektÃ¶rleri birbirine diktir**. 

<blockquote>

[](#green) âœ… $\langle \lambda_1 \lambda_2 v_1,v_2 \rangle = \langle v_1,\lambda_1 \lambda_2 v_2 \rangle \Leftrightarrow \langle \lambda_2 \mathbf{A} v_1,v_2 \rangle = \langle v_1,\lambda_1 \mathbf{A} v_2 \rangle = \langle \lambda_1 \mathbf{A}^{\intercal} v_1,v_2 \rangle$, $\mathbf{A}^\intercal = \mathbf{A}$ ve $\lambda_1 \neq \lambda_2$ olduÄŸundan eÅŸitlik sadece ve sadece $\langle v_1,v_2 \rangle = 0$ iÃ§in geÃ§erlidir.

</blockquote>

</blockquote>

â© Her $\mathbf{x} \in \mathbb{R}^n$ vektÃ¶rÃ¼ iÃ§in; $\mathbf{x}^{\intercal} \mathbf{A} \mathbf{x} > 0$ ÅŸartÄ±nÄ± saÄŸlayan matrislere **pozitif tanÄ±mlÄ±** denir. $\mathbf{x}^{\intercal} \mathbf{A} \mathbf{x} \geq 0$ ise **pozitif yarÄ± tanÄ±mlÄ±**, $\mathbf{x}^{\intercal} \mathbf{A} \mathbf{x} < 0$ geÃ§erliyse **negatif tanÄ±mlÄ±**, $\mathbf{x}^{\intercal} \mathbf{A} \mathbf{x} \leq 0$ geÃ§erliyse **negatif yarÄ± tanÄ±mlÄ±** denir. 

<blockquote>

[](#blue)

ğŸ“Œ $\mathbf{e}^{\intercal} \mathbf{A} \mathbf{e}$ ile matrisin kÃ¶ÅŸegenleri seÃ§ilebildiÄŸinden, pozitif tanÄ±mlÄ± matrislerde $\text{diag}(\mathbf{A}) > 0$, negatif tanÄ±mlÄ± matrislerde $\text{diag}(\mathbf{A}) < 0$ olmalÄ±dÄ±r. 

ğŸš¨ $\mathbf{x}$, $\mathbf{A}$'nÄ±n Ã¶zvektÃ¶rÃ¼ ise $\mathbf{x}^{\intercal} \mathbf{A} \mathbf{x} = \lambda \mathbf{x}^{\intercal}\mathbf{x}$ olduÄŸundan, $\lambda$'nÄ±n iÅŸareti matrisin tanÄ±mÄ±na baÄŸlÄ± belirlenir.

</blockquote>

<p style="text-align: center; font-family:cursive; page-break-before: always">OLASILIK ve RASTGELE DEÄÄ°ÅKENLER</p>

â© Bayes KuralÄ±: Bir $B$ olayÄ±nÄ±n olmasÄ± durumunda, $A$ olayÄ±nÄ±n olasÄ±lÄ±ÄŸÄ± aÅŸaÄŸÄ±daki ÅŸekilde tanÄ±mlÄ±dÄ±r.

$$
P(A \lvert B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \lvert A)P(A)}{P(B)}
$$

<blockquote>

[](#green)

ğŸ“Œ $A,B$ **baÄŸÄ±msÄ±z** olaylarsa; $P(A\cap B)=P(A)P(B)$ olacaÄŸÄ±ndan $P(A \lvert B) = P(A)$ olur.

ğŸ“Œ $A,B$ **baÄŸdaÅŸmaz** olaylarsa; $P(A\cap B)=0$ olacaÄŸÄ±ndan $P(A \lvert B) = 0$ olur.

ğŸ“Œ $B \subset A$ ise  $P(A\cap B) = P(B)$ olcaÄŸÄ±ndan $P(A \lvert B) = 1$ olur.

</blockquote>

â© **KÃ¼mÃ¼latif daÄŸÄ±lÄ±m** ( $F_{\mathbf{x}}(x)$ ) ve **olasÄ±lÄ±k yoÄŸunluk fonksiyonlarÄ±** ( $f_{\mathbf{x}}(x)$ ) ÅŸu ÅŸekilde tanÄ±mlanÄ±r: $P \lbrace  x_1 < \mathbf{x} \leq x_2  \rbrace  = F_{\mathbf{x}}(x_2) - F_{\mathbf{x}}(x_1) = \int_{x_1}^{x_2} f_{\mathbf{x}}(x)dx$

â© **Beklenen DeÄŸer:** $E[\mathbf{x}] = \int_{-\infty}^{\infty} x f_{\mathbf{x}}(x) dx$ ğŸ”” **DeÄŸiÅŸinti:** $\sigma^2_\mathbf{x} = \int_{-\infty}^{\infty} (x-\mu)^2 f_{\mathbf{x}}(x) dx = E[(x-\mu)^2]$


| DaÄŸÄ±lÄ±m AdÄ±          | KullanÄ±m AmacÄ±                     | Matematiksel GÃ¶sterim                                            | Ortalama            | Varyans               |
| -------------------- |------------------------------------| ---------------------------------------------------------------- | ------------------- | --------------------- |
| Normal        | TÃ¼m daÄŸÄ±lÄ±mlarÄ±n aritmetik ortalamasÄ±nÄ±n yakÄ±nsadÄ±ÄŸÄ± daÄŸÄ±lÄ±mdÄ±r | $\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ | $\mu$               | $\sigma^2$            |
| Exponansiyel  | BaÄŸÄ±msÄ±z olaylarÄ±n birbirini bekletme sÃ¼relerinin daÄŸÄ±lÄ±mÄ±dÄ±r                                  | $\lambda e^{-\lambda x}, \quad x \geq 0$                         | $\frac{1}{\lambda}$ | $\frac{1}{\lambda^2}$ |
| DÃ¼zgÃ¼n        | EÅŸit olasÄ±lÄ±klÄ± olaylarÄ±n daÄŸÄ±lÄ±mÄ±dÄ±r                                   | $\frac{1}{b-a}, \quad a < x < b$                                 | $\frac{a+b}{2}$     | $\frac{(b-a)^2}{12}$  |
| Binom        | $p$ olasÄ±lÄ±klÄ± olayÄ±n $n$ denemede gÃ¶rÃ¼lme sayÄ±sÄ±nÄ±n daÄŸÄ±lÄ±mÄ±dÄ±r                                   | ${n \choose k}p^k (1-p)^{(n-k)}$                                 | $np$                | $np(1-p)$             |
| Poisson      | $p \ll$ olasÄ±lÄ±klÄ± olayÄ±n $n \gg$ denemede gÃ¶rÃ¼lme sayÄ±sÄ±nÄ±n daÄŸÄ±lÄ±mÄ±dÄ±r                                   | $e^{-\lambda}\frac{\lambda^k}{k!}, \quad \lambda=np$                               | $\lambda$           | $\lambda$             |
| DÃ¼zgÃ¼n        | EÅŸit olasÄ±lÄ±klÄ± olaylarÄ±n daÄŸÄ±lÄ±mÄ±dÄ±r                                   | $\frac{1}{N}$                                                    | $\frac{N+1}{2}$     | $\frac{N^2-1}{12}$    |


â© $\mathbf{x},\mathbf{y}$ iki rastgele deÄŸiÅŸken olmak Ã¼zere; **korelasyon** $R_{\mathbf{xy}} = E[\mathbf{x}\mathbf{y}]$, **kovaryans** $C_{\mathbf{xy}} = E[(\mathbf{x}-\mu_\mathbf{x})(\mathbf{y}-\mu_\mathbf{y})]$ ÅŸeklinde tanÄ±mlanÄ±r.

<blockquote>

[](#blue)

ğŸ“Œ $C_{\mathbf{xy}} = 0$ yani $E[\mathbf{x}\mathbf{y}] = E[\mathbf{x}]E[\mathbf{y}]$ olan rastgele deÄŸiÅŸkenlere **iliÅŸkisiz** denir.  **BaÄŸÄ±msÄ±z**  rastgele deÄŸiÅŸkenler iÃ§in $P \lbrace x,y \rbrace  = P \lbrace x \rbrace P \lbrace y \rbrace$ olduÄŸundan **baÄŸÄ±msÄ±z rastgele deÄŸiÅŸkenler iliÅŸkisizdir**.

ğŸ“Œ Ä°ki rastgele deÄŸiÅŸkenin **korelasyonu $E[\mathbf{x}\mathbf{y}]=0$ ise bu deÄŸiÅŸkenler ortogonaldir** veya **birbirlerine diktir** denir.

</blockquote>

ğŸ’¥ğŸ’¥ **Merkezi Limit Teoremi:** $(\mu, \sigma^2)$ ile karakterize edilen bir daÄŸÄ±lÄ±mdan toplam $n$ Ã¶rnek alÄ±nmaktaysa, Ã¶rneklerin ortalamalarÄ±nÄ±n yakÄ±sadÄ±ÄŸÄ± daÄŸÄ±lÄ±m $\mathcal{N}(\mu, \sigma^2/n)$ ile Normal daÄŸÄ±lÄ±m olacaktÄ±r.

ğŸ“Œ Normal daÄŸÄ±lÄ±mÄ±n kÃ¼mÃ¼latif daÄŸÄ±lÄ±m fonksiyonun hesaplanmasÄ± $F_{\mathbf{x}}(x) = G\left( \frac{x-\mu}{\sigma}\right)$ formÃ¼lÃ¼ ile yapÄ±lÄ±r.


<p style="text-align: center; font-family:cursive; page-break-before: always">Ä°ÅARET Ä°ÅLEME</p>

ğŸ”‹ **Enerji** bir sistemin genliÄŸininin karesel toplamÄ±nÄ± $E = \int_{t_1}^{t_2} \lvert x(t) \lvert^2 dt$ gÃ¶stermektedir. âš¡ **GÃ¼Ã§** birim zamandaki enerji olarak tanÄ±mlanÄ±r ve $P(x) = \frac{E}{t_2-t_1}$ ile hesaplanÄ±r. **Sonlu enerjili sistemlerin ortalama gÃ¼cÃ¼ sÄ±fÄ±rdÄ±r**.

<blockquote>

[](#green)

- ğŸ¤¯ **Belleksiz** sistemlerde Ã§Ä±ktÄ± sadece o anki girdiye baÄŸlÄ±dÄ±r. 
- ğŸ§© **Terslenebilir** sistemlerde farklÄ± girdi farklÄ± bir Ã§Ä±ktÄ±ya neden olmaktadÄ±r.  
- âŒ› **Nedensel** sistemlerde Ã§Ä±ktÄ± girdinin o anki veya daha Ã¶nceki deÄŸerlerine baÄŸlÄ±dÄ±r. 
- ğŸ’£ **KararlÄ±** sistemlerde kÃ¼Ã§Ã¼k bir girdi, Ã§Ä±kÄ±ÅŸta sonlu bir etkiye neden olur. 
- ğŸ—¼**Zamanla DeÄŸiÅŸmeyen** sistemlerde belirli bir girdiye verilen Ã§Ä±ktÄ±, zamanla deÄŸiÅŸmez.
- ğŸ”› **DoÄŸrusal** sistem $a x_1(t) + b x_2(t)$ girdisi iÃ§in $a y_1(t) + b y_2(t)$ ÅŸeklinde Ã§Ä±ktÄ± Ã¼retmelidir.
  
</blockquote>

â© **DoÄŸrusal ve zamanla deÄŸiÅŸmeyen** bir sistemin Ã§Ä±ktÄ±sÄ±; $y[n] = \sum_{k=-\infty}^{\infty} x[k] h[n-k]$ **evriÅŸim** ile bulunabilir.


|Periyodik Ä°ÅŸaretler iÃ§in Fourier Serisi | SÃ¼rekli Sinyaller | AyrÄ±k Sinyaller |
|----|----|----|
| Fourier Seri KatsayÄ±larÄ± | $a_k = \frac{1}{T} \int_{T} x(t) e^{-jk(2\pi / T) t} dt$ | $a_k = \frac{1}{N} \sum_{n \in N} x[n] e^{-jk (2\pi / N) n}$ |
| Zaman Ä°ÅŸaretinin Elde Edilmesi | $x(t) = \sum_{k=-\infty}^{\infty} a_k e^{jk(2\pi / T) t}$ | $x[n] = \sum_{k \in N} a_k e^{jk (2\pi / N) n}$ |

<blockquote>

[](#blue) ğŸ’¥ğŸ’¥ **Mutlak toplamÄ± sonlu** olan, **sonlu sayÄ±da minima/maksima** iÃ§eren ve **sonlu sayÄ±da kesikliÄŸi** olan sinyaller, periyodik olmasa dahi $T \to \infty$ varsayÄ±larak Fourier seri katsayÄ±larÄ± bulunabilir. Ancak $T \to \infty$ olduÄŸundan katsayÄ±lar **sÃ¼rekli** olacaktÄ±r. Bu iÅŸleme **Fourier DÃ¶nÃ¼ÅŸÃ¼mÃ¼** denir.

</blockquote>

|Aperiyodik Ä°ÅŸaretler iÃ§in Fourier DÃ¶nÃ¼ÅŸÃ¼mÃ¼ | SÃ¼rekli Sinyaller | AyrÄ±k Sinyaller |
|----|----|----|
| Fourier DÃ¶nÃ¼ÅŸÃ¼mÃ¼ | $X(jw) = \int_{-\infty}^{\infty} x(t) e^{-jwt} dt$ | $X(e^{jw}) = \sum_{n=-\infty}^{\infty} x[n] e^{-jwn}$ |
| Ters Fourier DÃ¶nÃ¼ÅŸÃ¼mÃ¼ | $x(t) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} X(jw) e^{jwt} dw$ | $x[n] = \frac{1}{2 \pi} \int_{2\pi} X(e^{jw}) e^{jwn} dw$ |
| Laplace/$z$ DÃ¶nÃ¼ÅŸÃ¼mÃ¼ | $X(s) = \int_{-\infty}^{\infty} x(t) e^{-st} dt$ | $X(z) = \sum_{n=-\infty}^{\infty} x[n] z^{-n}$ |

ğŸ”¸$y(t) = x(t) \ast h(t) \overset{\mathcal{F}}{\longleftrightarrow} Y(jw) = X(jw) H(jw)$ ile hesaplanabilir.

ğŸ”¸**Parseval Teoremi:** $\int_{-\infty}^{\infty} \lvert x(t) \lvert^2 dt = \frac{1}{2 \pi} \int_{-\infty}^{\infty} \lvert X(jw) \lvert^2 dw$


â© Laplace dÃ¶nÃ¼ÅŸÃ¼mÃ¼ $s=\sigma + jw$ kullanÄ±larak, sÃ¼rekli Fourier dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼n, $z$ dÃ¶nÃ¼ÅŸÃ¼mÃ¼ $z=r e^{jw}$ kullanÄ±larak, ayrÄ±k Fourier dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼n genelleÅŸtirilmiÅŸ ÅŸeklidir. Bu dÃ¶nÃ¼ÅŸÃ¼mler iÃ§in girdinin **kararlÄ± olmasÄ± ÅŸart deÄŸildir**.

| ROC-Girdi Sinyali Ä°liÅŸkisi | Laplace DÃ¶nÃ¼ÅŸÃ¼mÃ¼ | $z$ DÃ¶nÃ¼ÅŸÃ¼mÃ¼ |
|------|------|------|
| KararlÄ±lÄ±k | ROC $\sigma = 0$ iÃ§ermelidir | ROC $\lvert z \lvert=1$ Ã§emberi iÃ§ermelidir |
| Nedensel | ROC $\sigma > \alpha$ ÅŸeklinde saÄŸ taraflÄ± olmalÄ±dÄ±r | ROC $\lvert z \lvert = \infty$ iÃ§ermelidir |
| KararlÄ± ve Nedensel | TÃ¼m kutuplar sol tarafta olmalÄ±dÄ±r | TÃ¼m kutuplar **birim Ã§emberin** iÃ§erisinde olmalÄ±dÄ±r |


<blockquote>

[](#green) ğŸš¨ DÃ¶nÃ¼ÅŸÃ¼m fonksiyonu **en az bir tane**, **sadeleÅŸme ile yok edilemeyen** kutup iÃ§ermesi durumunda girdi sinyali sonsuz uzunlukta olacaÄŸÄ±ndan, bu sinyallere **IIR** denir. **FIR** sistemlerde ise **sÄ±fÄ±r hariÃ§** kutup noktasÄ± bulunmamaktadÄ±r.

</blockquote>

â© **Ã–rnekleme** $x_s(t) = \sum_{n=-\infty}^{\infty} x(t) \delta(t - nT_s)$ sÃ¼rekli sinyalden Ã¶rnekler alÄ±nmasÄ± iÅŸlemidir. Bu durumda sinyalin frekans gÃ¶sterimi aÅŸaÄŸÄ±daki ÅŸekilde olacaktÄ±r.

$$
X_s(jw) = \frac{1}{T} \sum_{k=-\infty}^{\infty} X(jw) . 1e^{-jkw_s} = \frac{1}{T} \sum_{k=-\infty}^{\infty} X \left(j(w - kw_s) \right)
$$

ğŸ”” Ã–rneklenen sinyalin frekans uzayÄ±nda $w_s$ ile periyodik olduÄŸu gÃ¶rÃ¼lÃ¼r. 

<blockquote>

[](#blue) ğŸ’Š **Bant geniÅŸliÄŸi sÄ±nÄ±rlÄ± olan** sinyalden **sinyalin ihtiva ettiÄŸi en yÃ¼ksek frekanstan daha sÄ±k** Ã¶rnekler alÄ±nÄ±rsa sinyal **kayÄ±psÄ±z** geri Ã§atÄ±labilir.

âœ… **Nyquist-Shannon Ã–rnekleme Teoremi:** Maksimum frekansÄ± $w_m$ olan bir sinyalin bant geniÅŸliÄŸi $2w_m$ olacaktÄ±r. Ã–rnekleme sonrasÄ± sinyalin giriÅŸim yaÅŸamamasÄ± iÃ§in **Ã¶rnekleme frekansÄ± $w_s > 2w_m$ seÃ§ilmelidir**. Bu durumda orjinal sinyal Ã¶rneklerinden mÃ¼kemmel ÅŸekilde geri Ã§atÄ±labilecektir.

ğŸš¨ğŸš¨ Ã–rtÃ¼ÅŸme oluÅŸturmayacak **en dÃ¼ÅŸÃ¼k Ã¶rnekleme frekansÄ±** bant geniÅŸliÄŸi $B < w_m$ ile sÄ±nÄ±rlÄ± sinyaller iÃ§in daha **dÃ¼ÅŸÃ¼ktÃ¼r**: $w_s = \frac{2 w_m}{ \lfloor w_m / B \rfloor}$

</blockquote>

â© **TÃ¼m GeÃ§iren (All Pass)** sistemler **tÃ¼m frekans bileÅŸenlerine sabit bir kazanÃ§** uygular. $H(z)$ **transfer fonksiyonunun sÄ±fÄ±rlarÄ±nÄ±n da birim Ã§emberin iÃ§inde** olan sistemlere **Min Faz** sistem denir. Bu sistemlerin **tersi de nedensel ve kararlÄ±dÄ±r**.


â© Rastgele sÃ¼recin istatitistikleri zamanla deÄŸiÅŸmiyorsa **sÄ±kÄ± duraÄŸan**, sadece ortalama ve kovaryansÄ± deÄŸiÅŸmiyorsa **geniÅŸ anlamda duraÄŸan** sÃ¼reÃ§ denir.

ğŸ’Š SÄ±fÄ±r ortalamalÄ± $\mathbf{x}$ ve $\mathbf{y}$ vektÃ¶rleri arasÄ±ndaki **iliÅŸkiyi** kaldÄ±racak $\mathbf{H}$ dÃ¶nÃ¼ÅŸÃ¼m matrisi $\mathbf{e} = \mathbf{x} - \mathbf{H} \mathbf{y}$ olmak Ã¼zere $E[\mathbf{e} \mathbf{y}] = 0$ ÅŸartÄ±nÄ± saÄŸlamalÄ±dÄ±r. Bu da $\mathbf{H} = \mathbf{R}_ {xy} \mathbf{R}_ {yy}^{-1}$ seÃ§ilmesi durumunda olur.

> âœ… $E[\mathbf{e} \mathbf{y}] = E[(\mathbf{x} - \mathbf{H} \mathbf{y}) \mathbf{y}^\intercal] = E[\mathbf{x}\mathbf{y}^\intercal - \mathbf{H} \mathbf{y}\mathbf{y}^\intercal] = E[\mathbf{x}\mathbf{y}^\intercal] - \mathbf{H} E[\mathbf{y}\mathbf{y}^\intercal] = \mathbf{R}_ {xy} - \mathbf{H}\mathbf{R}_ {yy} = 0$


â© **GÃ¼Ã§ spektral yoÄŸunluÄŸu**; **sinyalin gÃ¼cÃ¼nÃ¼n frekans bileÅŸenlerine baÄŸlÄ± deÄŸiÅŸimini** gÃ¶steren bir deÄŸerdir. Bu deÄŸer $\mathbf{S}_x$ ile gÃ¶sterilir ve otokorelasyon matrisinin Fourier dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ile elde edilir.

$$
\mathbf{S}_ x(w) = \int_ {-\infty}^{\infty} \mathbf{R}_ x(\tau) e^{-jw\tau} d\tau
$$


ğŸ“ **Otoregresif modeller**; elimizde bulunan verinin geÃ§miÅŸ elemanlarÄ±nÄ±n aÄŸÄ±rlÄ±klÄ± toplamÄ± ile gÃ¼ncel deÄŸeri kestirmeye Ã§alÄ±ÅŸÄ±r.

ğŸ“ **Wiener filtre** problemi bir sisteme benzeterek girdi $z(t)$ ile Ã§Ä±ktÄ± $\hat{y}(t)$ arasÄ±ndaki dÃ¶nÃ¼ÅŸÃ¼mÃ¼ saÄŸlayan transfer fonksiyonunu bulmayÄ± hedefler. Burada $e(t) = \hat{y}(t+\lambda) - y(t+\lambda)$ hatasÄ±nÄ±n en kÃ¼Ã§Ã¼klenmesi iÃ§in $E[e(t) z(t)] = 0$ olmalÄ±dÄ±r. 

<p style="text-align: center; font-family:cursive; page-break-before: always">GÃ–RÃœNTÃœ Ä°ÅLEME</p>


â© **HIT** Ã§ekirdek matrisinin uygulandÄ±ÄŸÄ± bÃ¶lgede en az bir tane **1** bulunmasÄ± durumuna denir ve $\oplus$ sembolÃ¼ ile gÃ¶sterilir. ğŸŸ¨ **FIT** Ã§ekirdek matrisinin uygulandÄ±ÄŸÄ± bÃ¶lgedeki tÃ¼m deÄŸerlerin **1** olmasÄ± durumuna denir ve $\ominus$ sembolÃ¼ ile gÃ¶sterilir.

â© **GeniÅŸleme (Dilation)** Beyaz pikseller geniÅŸlemeye uÄŸradÄ±ÄŸÄ±ndan bu ÅŸekilde isimlendirilir, $A \oplus B$ iÅŸlemi ile gÃ¶sterilir. ğŸŸ© **Daralma (Erosion)** Beyaz pikseller daralmaya uÄŸradÄ±ÄŸÄ±ndan bu ÅŸekilde isimlendirilir, $A \ominus B$ iÅŸlemi ile gÃ¶sterilir.

â© **AÃ§ma (Opening)** bir imgenin Ã¶nce daralma, ardÄ±ndan **geniÅŸleme** iÅŸlemlerine tabii tutulmasÄ± iÅŸlemine denir, $(A \ominus B)\oplus B$ ile gÃ¶sterilir. ğŸŸ§ **Kapama (Closing)** bir imgenin Ã¶nce geniÅŸleme, ardÄ±ndan **daralma** iÅŸlemlerine tabii tutulmasÄ± iÅŸlemine denir, $(A \oplus B)\ominus B$ ile gÃ¶sterilir.

<blockquote>

[](#green) **SÄ±kÄ±ÅŸtÄ±rma** niÃ§in yapÄ±lÄ±r? ğŸ”¸ $N_\text{renk}< 2^{24}$ olduÄŸundan her piksel 24 bit yerine daha az bit ile ifade edilebilir ğŸ”¸ Uzamsal renk benzerliÄŸi kodlamada kullanÄ±labilir ğŸ”¸ AlgÄ±sal aÃ§Ä±dan Ã¶nemsiz noktalar daha az bitle gÃ¶sterilebilir.

</blockquote>

ğŸ“ **Huffman kodlama** sembol baÅŸÄ±na dÃ¼ÅŸen bit sayÄ±sÄ±nÄ± sembolÃ¼n gÃ¶rÃ¼lme olasÄ±lÄ±ÄŸÄ±na ters orantÄ±lÄ± ÅŸekilde ayarlayarak sÄ±kÄ±ÅŸtÄ±rma saÄŸlanÄ±r. 

ğŸ“ **Golomb kodlama** exponansiyel olarak azalan olasÄ±lÄ±ÄŸa sahip pozitif tam sayÄ± deÄŸerlerin kodlanmasÄ± iÃ§in Ã¶nerilen bir yÃ¶ntemdir. Geometrik daÄŸÄ±lÄ±mlÄ± bir sembol seti iÃ§in optimum sonucu Ã¼retmektedir. $x = q M + r$ yapÄ±lÄ±r, $q$ sayÄ±sÄ± $q+1$ bit ile, $r$ sayÄ±sÄ± ikili sistemde kodlanÄ±r.

ğŸ“ **LZW** kodlamada **farklÄ± uzunluktaki sembol gruplarÄ±na sabit uzunlukta kod sÃ¶zcÃ¼kleri atayarak** sÄ±kÄ±ÅŸtÄ±rma saÄŸlamaktadÄ±r.

<p style="text-align: center; font-family:cursive; page-break-before: always">OPTÄ°MÄ°ZASYON</p>

â©  Bir fonksiyonun en kÃ¼Ã§Ã¼k veya en bÃ¼yÃ¼k deÄŸerini aldÄ±ÄŸÄ± noktalara **ekstremum** noktalar denir. ğŸ’¥ Ekstremum noktalar iÃ§in $\boxed{\nabla f(\mathbf{x}^{\ast})^{\intercal} \Delta \mathbf{x} \geq 0}$ ÅŸartÄ± saÄŸlanmalÄ±dÄ±r.

<blockquote>

[](#blue) ğŸ’¥ğŸ’¥ Bir noktanÄ±n yerel ekstremum nokta olabilmesi iÃ§in $\nabla^2 f(\mathbf{x}^{\ast})$ **pozitif veya negatif yarÄ± tanÄ±mlÄ±** olmasÄ± gereklidir. $f(\mathbf{x}^{\ast}) = 0$ ÅŸartÄ±nÄ± saÄŸlayan ve belirsiz Hessian matrisine sahip noktalara **eÄŸer noktasÄ±** denir.

</blockquote>

â©  Bir fonksiyona aÅŸaÄŸÄ±daki ÅŸekilde yaklaÅŸÄ±mda bulunabilir. Sadece gradyan kullanÄ±larak yapÄ±lan yaklaÅŸÄ±ma **doÄŸrusal yaklaÅŸÄ±m**, ikinci tÃ¼revin de kullanÄ±ldÄ±ÄŸÄ± yaklaÅŸÄ±ma **kuadratik yaklaÅŸÄ±m** denir.

$$
\boxed{
f(\mathbf{y}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^{\intercal} (\mathbf{y}-\mathbf{x}) + \frac{1}{2}(\mathbf{y}-\mathbf{x})^{\intercal} \nabla^2 f(\mathbf{x}) (\mathbf{y}-\mathbf{x})}
$$

â© **Gradyan yÃ¶ntemleri** iteratif olarak, $\mathbf{x}_k$ vektÃ¶rÃ¼nÃ¼ duraÄŸan nokta olmaya yaklaÅŸtÄ±racak ÅŸekilde seÃ§ilen $\mathbf{d}_k$ iniÅŸ yÃ¶nÃ¼nde gÃ¼ncelleyerek  fonksiyonun duraÄŸan noktasÄ±nÄ± bulmaya Ã§alÄ±ÅŸan yÃ¶ntemlerdir. 

$$\mathbf{x}_{k+1} = \mathbf{x}_k + \eta \mathbf{d}_k$$

<blockquote>

[](#green) ğŸ””ğŸ”” Bir $\mathbf{d}$ vektÃ¶rÃ¼nÃ¼n **iniÅŸ yÃ¶nÃ¼ olabilmesi** iÃ§in $\boxed{\nabla f(\mathbf{x})^{\intercal} \mathbf{d} < 0}$ olmalÄ±dÄ±r.

</blockquote>

<blockquote>

ğŸ“Œ **Gradyan iniÅŸ yÃ¶ntemi** $\mathbf{d}_k$ vektÃ¶rÃ¼nÃ¼, gradyanÄ±n tam tersi yÃ¶nÃ¼nde, $\mathbf{d}_k=-\nabla f(\mathbf{x})$ seÃ§en yÃ¶ntemdir.

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \nabla f(\mathbf{x})
$$

ğŸ“Œ **Gauss-Newton yÃ¶ntemi** doÄŸrusal olmayan maliyet fonksiyonlarÄ±nÄ±nÄ±n ekstremum noktalarÄ±nÄ±, bu fonksiyonlarÄ±n **doÄŸrusal yaklaÅŸÄ±mlarÄ±nÄ±** kullanarak bulmayÄ± Ã§alÄ±ÅŸan bir yÃ¶ntemdir.

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \frac{1}{2}\left(\mathbf{J}(\mathbf{x}_k)^{\intercal}\mathbf{J}(\mathbf{x}_k) \right)^{-1} \nabla \mathbf{t}(\mathbf{x}_k)
$$

ğŸ“Œ **Levenberg-Marquardt yÃ¶ntemi** Gauss-Newton yÃ¶nteminin gradyan iniÅŸ yÃ¶ntemi ile arasÄ±ndaki ilÅŸikinin kullanÄ±lmasÄ± Ã¼zerine kurulmuÅŸtur. 

$$
\mathbf{x}_{k+1} = \left(\mathbf{J}(\mathbf{x}_k)^{\intercal}\mathbf{J}(\mathbf{x}_k) + \lambda \mathbf{I} \right)^{-1} \mathbf{J}(\mathbf{x}_k)^{\intercal} \mathbf{\hat{b}}
$$

ğŸ“Œ **Newton yÃ¶ntemi** bir fonksiyonun kÃ¶klerinin bulunmasÄ± iÃ§in kullanÄ±lan bir yÃ¶ntemdir. $g(\mathbf{x}) = \nabla f(\mathbf{x})$ ÅŸeklinde bir fonksiyon tanÄ±mlarsak, $g$ fonksiyonunun kÃ¶kleri $f$ fonksiyonunun ekstremum noktalarÄ± olacaktÄ±r.

$$
\boxed{\mathbf{x}_{k+1} = \mathbf{x}_k - \eta\left(\nabla^2 f(\mathbf{x})\right)^{-1}\nabla f(\mathbf{x})}
$$ 

ğŸ“Œ **YarÄ± Newton yÃ¶nteminde** Newton yÃ¶nteminde kullanÄ±lan ve hesaplama maliyeti yÃ¼ksek olan $(\nabla^2 f(\mathbf{x}))^{-1}$ yerine Secant yÃ¶ntemi ile bir yaklaÅŸÄ±m Ã¶nerilmektedir. 

$$
\mathbf{H} = \nabla^2 f(\mathbf{x}_ k) \approx \frac{\nabla f(\mathbf{x}_ k) - \nabla f(\mathbf{x}_ {k-1})}{\mathbf{x}_ k - \mathbf{x}_ {k-1}}
$$

</blockquote>

ğŸ“™ **KÄ±sÄ±tlÄ± Optimizasyon ve KKT**

$$
\min f(\mathbf{x}) \quad \text{s.t } \quad g_i(\mathbf{x}) \leq 0, \quad i=1,\dots,m \quad, \quad h_j(\mathbf{x}) = 0, \quad j=1,\dots,p \\
$$

ğŸ”” Burada $f,g_i,h_j$ tÃ¼revlenebilir fonksiyonlarÄ± sÄ±rasÄ±yla **ana problemi**, **eÅŸitsizlikleri** ve **eÅŸitlikleri** gÃ¶stermektedir. Bu ifade Lagrange Ã§arpanlarÄ± kullanÄ±larak aÅŸaÄŸÄ±daki ÅŸekilde yazÄ±labilir.

$$
L(\mathbf{x}, \lambda, \mu) = f(\mathbf{x}) + \sum_i \lambda_i g_i(\mathbf{x}) + \sum_j \mu_j h_j(\mathbf{x})
$$

<p style="text-align: center; font-family:cursive; page-break-before: always">Ã–RÃœNTÃœ TANIMA</p>

Bir veri seti Ã¼zerinde $A$ olayÄ±nÄ±n gÃ¶zlenmesi durumunda $B$'nin ne olabileceÄŸine dair kararÄ±;

<blockquote>

[](#blue)

ğŸŸ¢ $P(A \lvert B)$ olasÄ±lÄ±ÄŸÄ±nÄ± en bÃ¼yÃ¼kleyen $B$ deÄŸeri olarak veren yÃ¶nteme **En BÃ¼yÃ¼k Olabilirlikli Karar Verme (ML)** denir.
  
ğŸŸ¡ $P(B \lvert A) \sim P(A \lvert B)P(B)$ olasÄ±lÄ±ÄŸÄ±nÄ± en bÃ¼yÃ¼kleyen $B$ deÄŸerine gÃ¶re veren yÃ¶nteme **En BÃ¼yÃ¼k ArtÃ§Ä±l Karar Verme (MAP)** adÄ± verilir.

ğŸŸ£ $C(B)$ fonksiyonu $B$ kararÄ±nÄ±n maliyetini gÃ¶stermek Ã¼zere; kararÄ± $C(B)P(B \lvert A)$ maliyeti Ã¼zerinden verme iÅŸlemine **BayesÃ§i Karar Verme** denir.

</blockquote>

ğŸ“¢ğŸ—§$P(B)$ olasÄ±lÄ±klarÄ± eÅŸit daÄŸÄ±lÄ±mlÄ±ysa **MAP=ML** olmaktadÄ±r.

ğŸ’Š Hata $C(\theta) = (\theta - \hat{\theta})^2$ karesel hata ile Ã¶lÃ§Ã¼lmesi durumunda en kÃ¼Ã§Ã¼k ortalama karesel hata kestirimi $E[f(\theta \lvert \mathbf{X})]$ ile bulunur.

ğŸ“ **Parzen penceresi** yÃ¶nteminde sabit geniÅŸlikli bir pencere iÃ§erisindeki verinin olasÄ±lÄ±ÄŸÄ± $P(\mathbf{x} \in W)$' ya bakÄ±larak olasÄ±lÄ±k yoÄŸunluk fonksiyonu kestirilir.

ğŸ“ **En yakÄ±n komÅŸu yÃ¶ntemi** pencere hacmini sabit tutmak yerine, $P(\mathbf{x} \in W)$ olasÄ±lÄ±ÄŸÄ±nÄ± sabit tutacak geniÅŸlikteki deÄŸiÅŸken boyutlu pencereleri kullanarak  yoÄŸunluk fonksiyonunu kestirmektedir. Matematiksel ifade Parzen penceresi ile aynÄ±dÄ±r ve aÅŸaÄŸÄ±da verilmiÅŸtir.

$$
f_x(\mathbf{x}) = P(\mathbf{x} \in W) \frac{1}{\text{Volume}(W)}
$$

ğŸ“ **Temel BileÅŸen Analizi** verinin **en kÃ¼Ã§Ã¼k izdÃ¼ÅŸÃ¼m hatasÄ±** ile yansÄ±tÄ±labileceÄŸi alt uzayÄ± bulan bir algoritmadÄ±r. Bu dÃ¶nÃ¼ÅŸÃ¼m, aynÄ± zamanda **verinin en bÃ¼yÃ¼k deÄŸiÅŸintiyi** gÃ¶sterdiÄŸi eksen ile aynÄ± olmaktadÄ±r. 

> [](#green) ğŸ’Š $\mathbf{S}$ sÄ±fÄ±r ortalamalÄ± verinin kovaryansÄ± olmak Ã¼zere, temel bileÅŸen analizi $\mathbf{S}\mathbf{p} = \lambda \mathbf{p}$ Ã¶zdeÄŸer probleminin Ã§Ã¶zÃ¼mÃ¼ ile bulunur.

ğŸ“ **Fisher AyrÄ±ÅŸtÄ±rma Analizi** izdÃ¼ÅŸÃ¼mÃ¼n varyansÄ± yerine, $\mathbf{S}_b{\mathbf{S}_w}^{-1}$ deÄŸerini en bÃ¼yÃ¼kler. $\mathbf{S}_b = \sum_i (\mu_i - \mu)$ sÄ±nÄ±flar arasÄ± varyansÄ± (sÄ±nÄ±f ortalamalarÄ±nÄ±n varyansÄ±), $\mathbf{S}_w = \sum_i E[(x_i - \mu_i)^2]$ sÄ±nÄ±flarÄ±n kendi iÃ§erisindeki varyanslarÄ±nÄ±n toplamÄ±nÄ± gÃ¶stermektedir.

> [](#green) ğŸ’Š Fisher AyrÄ±ÅŸtÄ±rma vektÃ¶rleri $\mathbf{S}_b\mathbf{p} = \lambda {\mathbf{S}_w}\mathbf{p}$ eÅŸitliÄŸinin Ã§Ã¶zÃ¼lmesi ile bulunur.

<blockquote>

[](#red) ğŸš¨ğŸš¨ Fisher ayrÄ±ÅŸtÄ±rma analizinde **sÄ±nÄ±f ortalamalarÄ± aynÄ±/yakÄ±n ise** $\mathbf{S}_b \approx 0$ olacaÄŸÄ±ndan, bulunan **sonuÃ§lar hatalÄ±** olacaktÄ±r.

</blockquote>

ğŸ“— **DoÄŸrusal AyÄ±rtaÃ§** fonksiyonlarÄ± $g(\mathbf{x}) = \mathbf{w}^{\intercal}\mathbf{x}$ ÅŸeklinde yazÄ±labilen fonksiyonlardÄ±r. Bu fonksiyonlarda $f(\mathbf{x} \in C_1) > 0$ ve $f(\mathbf{x} \in C_2) < 0$ olmasÄ± istendiÄŸinden;   $\mathbf{x}_i = -\mathbf{x}_i,\phantom{+} \mathbf{x}_i \in C_2$ tanÄ±mÄ± yapÄ±ldÄ±ÄŸÄ±nda doÄŸru sÄ±nÄ±flandÄ±rÄ±lan her Ã¶rnek iÃ§in $g(\mathbf{x}) = \mathbf{w}^{\intercal}\mathbf{x} > 0$ olacaktÄ±r.

ğŸ“Œ **Perceptron Ã–ÄŸrenmede** aÄŸÄ±rlÄ±k vektÃ¶rÃ¼nÃ¼n hata miktarÄ± $M$ yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±lan Ã¶rnekler Ã¼zerinden $f(\mathbf{w}) = \sum_{\mathbf{x}_i \in M} -\mathbf{w}^{\intercal}\mathbf{x}_i$ hata fonksiyonu tanÄ±mlanabilir ve gradyan iniÅŸ yÃ¶ntemi ile en kÃ¼Ã§Ã¼klenebilir. 

$$
\mathbf{w}_ {k+1} = \mathbf{w}_ {k} - \eta \nabla f(\mathbf{w}) = \mathbf{w}_ {k} + \eta \sum_{\mathbf{x}_ i \in M} \mathbf{x}_ {i}
$$

ğŸ“˜ **En KÃ¼Ã§Ã¼k Karesel Hata** $f(\mathbf{w}) = \sum_{i} (\mathbf{w}^{\intercal}\mathbf{x}_i - \mathbf{b}_i)^2$ ÅŸeklinde tanÄ±mlanÄ±r. $f(\mathbf{w})$ fonksiyonunun gradyanÄ± sÄ±fÄ±ra eÅŸitlenerek $\mathbf{w}$ Ã§Ã¶zÃ¼lebilir.

$$
\begin{aligned}
    \nabla f(\mathbf{w}) &= \nabla\sum_{i} (\mathbf{w}^{\intercal}\mathbf{x}_i - \mathbf{b}_i)^2 &= 2 (\mathbf{w}^{\intercal}\mathbf{x}_i - \mathbf{b}_i)\mathbf{x}_i &= 0\\
\end{aligned}
$$

ğŸ”¹ **DoÄŸrudan Ã‡Ã¶zÃ¼m:** Elde edilen gradyan ifadesi matris biÃ§inde yazÄ±ldÄ±ÄŸÄ±nda $\mathbf{X}^\intercal \mathbf{X} \mathbf{w} - \mathbf{b} \Rightarrow \mathbf{w} = (\mathbf{X}^\intercal \mathbf{X})^{-1}\mathbf{b}$ Ã§Ã¶zÃ¼mÃ¼ elde edilir. $(\mathbf{X}^\intercal \mathbf{X})^{-1}$ sÃ¶zde matris tersi iÅŸlemi stabil olmayabilir veya Ã§ok iÅŸlem gÃ¼cÃ¼ gerektirebilir.

ğŸ”¹ **Widrow-Hoff Ã‡Ã¶zÃ¼mÃ¼:** Bu yaklaÅŸÄ±m elde edilen gradyan ifadesi gradyan iniÅŸ yÃ¶nteminde kullanÄ±lÄ±r. $\mathbf{w}_ {k+1} = \mathbf{w}_ {k} - \eta (\mathbf{w}^{\intercal}\mathbf{x}_ i - \mathbf{b}_ i)\mathbf{x}_ i$

ğŸ”¹ **Ho-Kashyap Ã‡Ã¶zÃ¼mÃ¼:** Bu yaklaÅŸÄ±m $\mathbf{w}$ deÄŸiÅŸkenine ek olarak $\mathbf{b}$ iÃ§in de gradyan iniÅŸ yÃ¶ntemi kullanmayÄ± Ã¶nerir. DiÄŸer iki yÃ¶ntemde $\mathbf{b}$ rastgele bir pozitif vektÃ¶r seÃ§ildiÄŸinden bu yaklaÅŸÄ±m uzun sÃ¼rede Ã§alÄ±ÅŸmakta ancak daha iyi sonuÃ§lar Ã¼retmektedir.

<blockquote>

[](#blue) **En KÃ¼Ã§Ã¼k Karesel Hata** bÃ¼yÃ¼k hatalara daha Ã§ok ceza yÃ¼klediÄŸinden, aykÄ±rÄ± deÄŸerlere daha duyarlÄ± olmakta ve sonucu aykÄ±rÄ± deÄŸere doÄŸru yaklaÅŸtÄ±rmaktadÄ±r.

</blockquote>

ğŸ“™ **Destek VektÃ¶r Makineleri** sadece ayrÄ±ÅŸtÄ±rÄ±cÄ± **dÃ¼zleme en yakÄ±n** Ã¶rneklerin (destek vektÃ¶rleri) mesafelerini belirli bir $\mathbf{b} = 1 / \lVert \mathbf{w} \lVert$ deÄŸerine zorlamaktadÄ±r.

$\mathbf{b} = 1 / \lVert \mathbf{w} \lVert$ olmasÄ± durumunda noktalarÄ±n ayrÄ±ÅŸtÄ±rÄ±cÄ± dÃ¼zleme uzaklÄ±ÄŸÄ± $\lvert \mathbf{w}^{\intercal}\mathbf{x} + \mathbf{w}_0\lvert \geq 1$ olacaktÄ±r. 

Verinin $\mathbf{x}_i \in C_1$ iÃ§in $z_i = +1$,  $\mathbf{x}_i \in C_2$ iÃ§in $z_i = -1$ ÅŸeklinde etikenlendiÄŸini varsayarsak, yukarÄ±daki bu ifade $\mathbf{z} (\mathbf{w}^{\intercal}\mathbf{x} + \mathbf{w}_0) \geq 1$ ÅŸeklinde de yazÄ±labilir.

Bu iki bilinen bir optimizasyon problemi olarak yazÄ±ldÄ±ÄŸÄ±nda, destek vektÃ¶r makineleri yÃ¶nteminin maliyet fonksiyonu elde edilir.

$$
f(\mathbf{w}) = \frac{1}{2} \lVert \mathbf{w}\lVert^2 - \sum_i \alpha_i \left( z_i (\mathbf{w}^{\intercal}\mathbf{x}_i + \mathbf{w}_0) - 1 \right)
$$

